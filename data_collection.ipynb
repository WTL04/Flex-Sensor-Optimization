{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Different ML Models on Flex Sensor Bend Data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m√ó\u001b[0m This environment is externally managed\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try 'pacman -S\n",
      "\u001b[31m   \u001b[0m python-xyz', where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using 'python -m venv path/to/venv'.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have python-pipx\n",
      "\u001b[31m   \u001b[0m installed via pacman.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model training & pipelines\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, WhiteKernel, ConstantKernel as C\n",
    "# from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection ##\n",
    "\n",
    "In this notebook, the data I have here is from Wally's work, Besufekad's, and some simulated data I made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'flex_sensor_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[32m      2\u001b[39m file1 = \u001b[33m\"\u001b[39m\u001b[33mflex_sensor_data.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df1 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Concatenate the DataFrames\u001b[39;00m\n\u001b[32m      7\u001b[39m df = pd.concat([df1], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# `ignore_index=True` resets the index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/gpt/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/gpt/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/gpt/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/gpt/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/gpt/venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'flex_sensor_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "file1 = \"datasets/flex_sensor_data.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df = pd.concat([df1], ignore_index=True)  # `ignore_index=True` resets the index\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"Data types:\\n\", df.dtypes)\n",
    "\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep ##\n",
    "\n",
    "We will be testing to see if the model can accurately predict the bending angle of the sensor based on resistance readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['Angle (degrees)'])  # Replace 'TargetColumn' with the actual column name\n",
    "y = df['Angle (degrees)']\n",
    "\n",
    "# Split data into training and test sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (optional, based on sensor value range)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models ##\n",
    "\n",
    "The models that will be tested in this notebook:\n",
    "* Baseline models:\n",
    "    * Random Forest\n",
    "    * Linear Regression\n",
    "    * Polynomial Regression\n",
    "* Advance models:\n",
    "    * XGBoost Regression\n",
    "    * Gaussian Process Regression\n",
    "    * Model Generalization (combining all models together)\n",
    "\n",
    "## Evaluation ##\n",
    "\n",
    "Metrics to be used:\n",
    "* Mean squared error (MSE) - measures error magnitude\n",
    "* R**2 - evaluates model accuracy\n",
    "* Prediction curve evaluation - plots actual vs. predicted bending angles to observe trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Number of trees\n",
    "    'max_depth': [10, 20, 30, None],  # Tree depth\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples to split\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples in a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features per split\n",
    "    'bootstrap': [True, False]  # Bootstrapping\n",
    "}\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define parameter space for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700],  # More options for better tuning\n",
    "    'max_depth': [10, 20, 30, 50, 60, None],  # None allows fully grown trees\n",
    "    'min_samples_split': [2 ,3, 4, 5, 6],  # More splits to avoid overfitting\n",
    "    'min_samples_leaf': [1, 2, 4 , 5, 6],  # Controls complexity\n",
    "    'max_features': ['sqrt', 'log2', None],  # Feature selection strategies\n",
    "    'bootstrap': [True, False]  # Bootstrapping options\n",
    "}\n",
    "\n",
    "# Randomized Search with Cross Validation (5-fold)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model, param_distributions=param_dist,\n",
    "    n_iter=20,  # Randomly test 20 different combinations\n",
    "    scoring='r2', cv=5, n_jobs=-1, verbose=2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit Randomized Search to training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"\\nBest Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train Random Forest with best parameters\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "n, p = X_test.shape\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(\"\\nüå≤ Random Forest Performance:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "print(f\"Adjusted R¬≤: {adjusted_r2:.4f}\")\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(best_rf_model, X, y, cv=5, scoring='r2')\n",
    "cv_preds = cross_val_predict(best_rf_model, X, y, cv=5)\n",
    "cv_rmse = np.sqrt(mean_squared_error(y, cv_preds))\n",
    "cv_mae = mean_absolute_error(y, cv_preds)\n",
    "cv_r2 = r2_score(y, cv_preds)\n",
    "\n",
    "print(f\"\\nüîÅ Cross-Validation (5-Fold):\")\n",
    "print(f\"R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"Average R¬≤: {cv_scores.mean():.4f}\")\n",
    "print(f\"CV RMSE: {cv_rmse:.4f}\")\n",
    "print(f\"CV MAE: {cv_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Training Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost Regressor\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 300, 500, 700],  # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # How much each tree contributes\n",
    "    'max_depth': [3, 5, 7, 10],  # Complexity control\n",
    "    'min_child_weight': [1, 3, 5],  # Minimum weight needed for a new node split\n",
    "    'gamma': [0, 0.1, 0.3, 0.5],  # Regularization parameter\n",
    "    'colsample_bytree': [0.5, 0.7, 1],  # Percentage of features used per tree\n",
    "    'subsample': [0.6, 0.8, 1]  # Fraction of samples per tree\n",
    "}\n",
    "\n",
    "# Randomized Search with Cross Validation (5-fold)\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb_model, param_distributions=param_dist,\n",
    "    n_iter=20,  # Randomly test 20 different combinations\n",
    "    scoring='r2', cv=5, n_jobs=-1, verbose=2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit Randomized Search to training data\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"\\nBest Hyperparameters:\", random_search_xgb.best_params_)\n",
    "\n",
    "# Train XGBoost with best parameters\n",
    "best_xgb_model = random_search_xgb.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "n, p = X_test.shape\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(\"\\nüå≤ XGBoost Performance:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "print(f\"Adjusted R¬≤: {adjusted_r2:.4f}\")\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(best_xgb_model, X, y, cv=5, scoring='r2')\n",
    "cv_preds = cross_val_predict(best_xgb_model, X, y, cv=5)\n",
    "cv_rmse = np.sqrt(mean_squared_error(y, cv_preds))\n",
    "cv_mae = mean_absolute_error(y, cv_preds)\n",
    "cv_r2 = r2_score(y, cv_preds)\n",
    "\n",
    "print(f\"\\nüîÅ Cross-Validation (5-Fold):\")\n",
    "print(f\"R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"Average R¬≤: {cv_scores.mean():.4f}\")\n",
    "print(f\"CV RMSE: {cv_rmse:.4f}\")\n",
    "print(f\"CV MAE: {cv_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2 = r2_score(y_test, y_pred_lr)\n",
    "n, p = X_test.shape\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(\"\\nüìâ Linear Regression Performance:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "print(f\"Adjusted R¬≤: {adjusted_r2:.4f}\")\n",
    "\n",
    "# Coefficients\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': lr_model.coef_})\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(coef_df)\n",
    "print(f\"Intercept: {lr_model.intercept_:.4f}\")\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(lr_model, X, y, cv=5, scoring='r2')\n",
    "cv_preds = cross_val_predict(lr_model, X, y, cv=5)\n",
    "cv_rmse = np.sqrt(mean_squared_error(y, cv_preds))\n",
    "cv_mae = mean_absolute_error(y, cv_preds)\n",
    "cv_r2 = r2_score(y, cv_preds)\n",
    "\n",
    "print(f\"\\nüîÅ Cross-Validation (5-Fold):\")\n",
    "print(f\"R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"Average R¬≤: {cv_scores.mean():.4f}\")\n",
    "print(f\"CV RMSE: {cv_rmse:.4f}\")\n",
    "print(f\"CV MAE: {cv_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "degree = 4\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "poly_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_poly = poly_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred_poly)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_poly)\n",
    "r2 = r2_score(y_test, y_pred_poly)\n",
    "n, p = X_test.shape\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Output results\n",
    "print(\"\\nüî¢ Polynomial Regression (Degree 4) Performance:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "print(f\"Adjusted R¬≤: {adjusted_r2:.4f}\")\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(poly_model, X, y, cv=5, scoring='r2')\n",
    "cv_preds = cross_val_predict(poly_model, X, y, cv=5)\n",
    "cv_rmse = np.sqrt(mean_squared_error(y, cv_preds))\n",
    "cv_mae = mean_absolute_error(y, cv_preds)\n",
    "cv_r2 = r2_score(y, cv_preds)\n",
    "\n",
    "print(f\"\\nüîÅ Cross-Validation (5-Fold):\")\n",
    "print(f\"R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"Average R¬≤: {cv_scores.mean():.4f}\")\n",
    "print(f\"CV RMSE: {cv_rmse:.4f}\")\n",
    "print(f\"CV MAE: {cv_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try RationalQuadratic (can model multiple scales of variation)\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RationalQuadratic(length_scale=1.0, alpha=1.0) + WhiteKernel()\n",
    "\n",
    "# Initialize the Gaussian Process Regressor\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gpr.fit(X_train, y_train)\n",
    "\n",
    "# Predict mean and standard deviation\n",
    "y_pred_gpr, y_std_gpr = gpr.predict(X_test, return_std=True)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred_gpr)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_gpr)\n",
    "r2 = r2_score(y_test, y_pred_gpr)\n",
    "n, p = X_test.shape\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(\"\\nüìà Gaussian Process Regression Performance:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "print(f\"Adjusted R¬≤: {adjusted_r2:.4f}\")\n",
    "\n",
    "# # Cross-Validation\n",
    "# cv_scores = cross_val_score(gpr, X, y, cv=5, scoring='r2')\n",
    "# cv_preds = cross_val_predict(gpr, X, y, cv=5)\n",
    "# cv_rmse = np.sqrt(mean_squared_error(y, cv_preds))\n",
    "# cv_mae = mean_absolute_error(y, cv_preds)\n",
    "# cv_r2 = r2_score(y, cv_preds)\n",
    "\n",
    "# print(f\"\\nüîÅ Cross-Validation (5-Fold):\")\n",
    "# print(f\"R¬≤ Scores: {cv_scores}\")\n",
    "# print(f\"Average R¬≤: {cv_scores.mean():.4f}\")\n",
    "# print(f\"CV RMSE: {cv_rmse:.4f}\")\n",
    "# print(f\"CV MAE: {cv_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Comparison ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "# 1. Linear Regression\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.scatter(y_test, y_pred_lr, color='blue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.title(f'Linear Regression\\nR¬≤ = {r2_score(y_test, y_pred_lr):.4f}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Polynomial Regression (Deg 4)\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.scatter(y_test, y_pred_poly, color='green', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.title(f'Polynomial Regression (Deg 4)\\nR¬≤ = {r2_score(y_test, y_pred_poly):.4f}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. XGBoost\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.scatter(y_test, y_pred_xgb, color='darkorange', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.title(f'XGBoost Regression\\nR¬≤ = {r2_score(y_test, y_pred_xgb):.4f}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True)\n",
    "\n",
    "# 4. Random Forest\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.scatter(y_test, y_pred_rf, color='purple', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.title(f'Random Forest Regression\\nR¬≤ = {r2_score(y_test, y_pred_rf):.4f}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True)\n",
    "\n",
    "# 6. Gaussian Process Regression\n",
    "plt.subplot(2, 4, 6)\n",
    "plt.scatter(y_test, y_pred_gpr, color='deepskyblue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.title(f'GPR (RationalQuadratic)\\nR¬≤ = {r2_score(y_test, y_pred_gpr):.4f}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True)\n",
    "\n",
    "# # 7. Empty slot (optional use later)\n",
    "# plt.subplot(2, 4, 7)\n",
    "# plt.axis('off')\n",
    "\n",
    "plt.suptitle('Model Prediction Comparison (Actual vs Predicted)', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "n_models = 10  # number of XGBoost models\n",
    "xgb_models = []\n",
    "xgb_preds = []\n",
    "\n",
    "# Train models on bootstrapped data\n",
    "for i in range(n_models):\n",
    "    # Bootstrap resample the training data\n",
    "    X_boot, y_boot = resample(X_train, y_train, replace=True, random_state=42 + i)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42 + i,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_boot, y_boot)\n",
    "    xgb_models.append(model)\n",
    "    \n",
    "    # Predict on test set\n",
    "    pred = model.predict(X_test)\n",
    "    xgb_preds.append(pred)\n",
    "\n",
    "# Stack predictions: shape (n_models, n_samples)\n",
    "xgb_preds = np.array(xgb_preds)\n",
    "\n",
    "# Compute mean and std (uncertainty)\n",
    "xgb_pred_mean = np.mean(xgb_preds, axis=0)\n",
    "xgb_pred_std = np.std(xgb_preds, axis=0)\n",
    "\n",
    "# Now xgb_pred_mean is your final XGBoost prediction\n",
    "# xgb_pred_std is your uncertainty estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Predict on test set with all models\n",
    "lin_pred = lr_model.predict(X_test)\n",
    "poly_pred = poly_model.predict(X_test)\n",
    "rf_all_preds = np.array([tree.predict(X_test) for tree in best_rf_model.estimators_])\n",
    "rf_pred = np.mean(rf_all_preds, axis=0)\n",
    "xgb_pred = xgb_pred_mean\n",
    "gpr_pred, gpr_std = gpr.predict(X_test, return_std=True)\n",
    "\n",
    "# Estimate uncertainty\n",
    "lin_uncertainty = np.full_like(lin_pred, np.std(y_train - lr_model.predict(X_train)))\n",
    "poly_uncertainty = np.full_like(poly_pred, np.std(y_train - poly_model.predict(X_train)))\n",
    "rf_uncertainty = np.std(rf_all_preds, axis=0)\n",
    "xgb_uncertainty = xgb_pred_std\n",
    "gpr_uncertainty = gpr_std\n",
    "\n",
    "# Combine into DataFrame\n",
    "meta_df = pd.DataFrame({\n",
    "    'Resistance': X_test.flatten(),\n",
    "    'Lin_Pred': lin_pred,\n",
    "    'Lin_Uncertainty': lin_uncertainty,\n",
    "    'Poly_Pred': poly_pred,\n",
    "    'Poly_Uncertainty': poly_uncertainty,\n",
    "    'RF_Pred': rf_pred,\n",
    "    'RF_Uncertainty': rf_uncertainty,\n",
    "    'XGB_Pred': xgb_pred,\n",
    "    'XGB_Uncertainty': xgb_uncertainty,\n",
    "    'GPR_Pred': gpr_pred,\n",
    "    'GPR_Uncertainty': gpr_uncertainty,\n",
    "    'True_Angle': y_test.values\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "meta_df.to_csv(\"meta_model_input.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
